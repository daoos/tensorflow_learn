{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import urllib2\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import httplib\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(url, path):\n",
    "    headers = {'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'}\n",
    "    try:\n",
    "        request = urllib2.Request(url, headers=headers)\n",
    "        response = urllib2.urlopen(request)\n",
    "        content = response.read().decode('gbk')\n",
    "        # print content\n",
    "        pattern = re.compile('<div class=\"post_content_main\" id=\"epContentLeft\">.*?<h1>(.*?)</h1>', re.S)\n",
    "        title = re.findall(pattern=pattern, string=content)\n",
    "        if not title:\n",
    "            return\n",
    "        title = title[0]\n",
    "        # print title   # 标题\n",
    "        pattern = re.compile('<p class=\"otitle\">(.*?)<!-- AD200x300_2 -->', re.S)\n",
    "        text = re.findall(pattern=pattern, string=content)\n",
    "        # print text\n",
    "        if not text:\n",
    "            return\n",
    "        text = text[0]\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        clear_text = ''\n",
    "        for item in soup.find_all('p'):\n",
    "            t = item.get_text()\n",
    "            if t:\n",
    "                clear_text += (t + '\\n')\n",
    "        # print clear_text    # 正文\n",
    "        file_path = path + url[url.rfind('/')+1:url.rfind('.')] + '.txt'\n",
    "        f = open(file_path, mode='w')\n",
    "        f.write('原始链接：'+url+'\\n\\n')\n",
    "        f.write(title+'\\n\\n')\n",
    "        f.write(clear_text)\n",
    "        f.close()\n",
    "    except urllib2.URLError, e:\n",
    "        if hasattr(e, \"code\"):\n",
    "            print e.code\n",
    "        if hasattr(e, \"reason\"):\n",
    "            print e.reason\n",
    "\n",
    "\n",
    "def crawl_topic(url, topic):\n",
    "    print '正在爬取主题：', topic\n",
    "    print '链接：', url\n",
    "    topic_path = main_dictionary+topic+'\\\\'\n",
    "    if not os.path.exists(topic_path):\n",
    "        os.mkdir(topic_path)\n",
    "    headers = {'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'}\n",
    "    try:\n",
    "        request = urllib2.Request(url, headers=headers)\n",
    "        response = urllib2.urlopen(request)\n",
    "        content = response.read().decode('gbk')\n",
    "        # print content\n",
    "        pattern = re.compile('<td class=\".*?\"><span>.*?</span><a href=\"(.*?)\">(.*?)</a></td>')\n",
    "        items = re.findall(pattern=pattern, string=content)\n",
    "        pattern = re.compile('<td class=\".*?\"><a href=\"(.*?)\">(.*?)</a></td>')\n",
    "        items += re.findall(pattern=pattern, string=content)\n",
    "        for i, (link, title) in enumerate(items):\n",
    "            print link, title\n",
    "            write_file(link, topic_path)\n",
    "    except urllib2.URLError, e:\n",
    "        if hasattr(e, \"code\"):\n",
    "            print e.code\n",
    "        if hasattr(e, \"reason\"):\n",
    "            print e.reason\n",
    "    print '\\n\\n\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main_dictionary = './Content/'\n",
    "    if not os.path.exists(main_dictionary):\n",
    "        os.mkdir(main_dictionary)\n",
    "    url = 'http://news.163.com/rank/'\n",
    "    headers = {'User-Agent':'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'}\n",
    "    try:\n",
    "        print '建立连接'\n",
    "        request = urllib2.Request(url, headers={})\n",
    "        response = urllib2.urlopen(request)\n",
    "        content = response.read().decode('gbk')\n",
    "        # print content\n",
    "        pattern = re.compile(u'<div class=\"list\"><ul id=\"calendarList\"><li></li></ul></div>'\n",
    "                             u'.*?<div class=\"subNav\">快速跳转：(.*?)<div class=\"area areabg1\">', re.S)\n",
    "        link_name = re.findall(pattern=pattern, string=content)[0]\n",
    "        soup = BeautifulSoup(link_name, 'html.parser')\n",
    "        for item in soup.find_all('a'):\n",
    "            crawl_topic(item['href'], item.get_text())\n",
    "            break\n",
    "    except urllib2.URLError, e:\n",
    "        if hasattr(e,\"code\"):\n",
    "            print e.code\n",
    "        if hasattr(e,\"reason\"):\n",
    "            print e.reason\n",
    "    except httplib.IncompleteRead, e:\n",
    "        print '---httplib.IncompleteRead---'\n",
    "        print e\n",
    "        if hasattr(e, \"code\"):\n",
    "            print e.code\n",
    "        if hasattr(e, \"reason\"):\n",
    "            print e.reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
