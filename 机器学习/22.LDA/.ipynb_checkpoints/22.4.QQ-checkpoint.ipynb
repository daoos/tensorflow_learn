{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint\n",
    "import time\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.posseg\n",
    "\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_stopword():\n",
    "    f_stop = open('stopword.txt')\n",
    "    sw = [line.strip() for line in f_stop]\n",
    "    f_stop.close()\n",
    "    return sw\n",
    "\n",
    "\n",
    "def clean_info(info):\n",
    "    replace_str = (('\\n', ''), ('\\r', ''), (',', '，'), ('[表情]', ''))\n",
    "    for rs in replace_str:\n",
    "        info = info.replace(rs[0], rs[1])\n",
    "\n",
    "    at_pattern = re.compile(r'(@.* )')\n",
    "    at = re.findall(pattern=at_pattern, string=info)\n",
    "    for a in at:\n",
    "        info = info.replace(a, '')\n",
    "    idx = info.find('@')\n",
    "    if idx != -1:\n",
    "        info = info[:idx]\n",
    "    return info\n",
    "\n",
    "\n",
    "def regularize_data(file_name):\n",
    "    time_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2} \\d{1,2}:\\d{1,2}:\\d{1,2}')\n",
    "    qq_pattern1 = re.compile(r'([1-9]\\d{4,15})')    # QQ号最小是10000\n",
    "    qq_pattern2 = re.compile(r'(\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)*)')\n",
    "    f = open(file_name)\n",
    "    f_output = open(u'QQ_chat.csv', mode='w')\n",
    "    f_output.write('QQ,Time,Info\\n')\n",
    "    qq = chat_time = info = ''\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            t = re.findall(pattern=time_pattern, string=line)\n",
    "            qq1 = re.findall(pattern=qq_pattern1, string=line)\n",
    "            qq2 = re.findall(pattern=qq_pattern2, string=line)\n",
    "            if (len(t) >= 1) and ((len(qq1) >= 1) or (len(qq2) >= 1)):\n",
    "                if info:\n",
    "                    info = clean_info(info)\n",
    "                    if info:\n",
    "                        info = '%s,%s,%s\\n' % (qq, chat_time, info)\n",
    "                        f_output.write(info)\n",
    "                        info = ''\n",
    "                if len(qq1) >= 1:\n",
    "                    qq = qq1[0]\n",
    "                else:\n",
    "                    qq = qq2[0][0]\n",
    "                chat_time = t[0]\n",
    "            else:\n",
    "                info += line\n",
    "    f.close()\n",
    "    f_output.close()\n",
    "\n",
    "\n",
    "def load_stopwords():\n",
    "    stopwords = set()\n",
    "    f = open('stopword.txt')\n",
    "    for w in f:\n",
    "        stopwords.add(w.strip().decode('GB18030'))\n",
    "    f.close()\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def segment():\n",
    "    stopwords = load_stopwords()\n",
    "    data = pd.read_csv('QQ_chat.csv', header=0)\n",
    "    for i, info in enumerate(data['Info']):\n",
    "        info_words = []\n",
    "        for word, pos in jieba.posseg.cut(info):\n",
    "            if pos in ['n', 'nr', 'ns', 'nt', 'nz', 's', 't', 'v', 'vd', 'vn', 'z', 'a', 'ad', 'an', 'f', 'i', 'j', 'Ng']:\n",
    "                if word not in stopwords:\n",
    "                    info_words.append(word.encode('utf-8'))\n",
    "        if info_words:\n",
    "            data.iloc[i, 2] = ' '.join(info_words)\n",
    "        else:\n",
    "            data.iloc[i, 2] = np.nan\n",
    "    data.dropna(axis=0, how='any', inplace=True)\n",
    "    data.to_csv('QQ_chat_segment.csv', sep=',', header=True, index=False)\n",
    "\n",
    "\n",
    "def combine():\n",
    "    data = pd.read_csv('QQ_chat_segment.csv', header=0)\n",
    "    data['QQ'] = pd.Categorical(data['QQ']).codes\n",
    "    f_output = open('QQ_chat_result.csv', mode='w')\n",
    "    f_output.write('QQ,Info\\n')\n",
    "    for qq in data['QQ'].unique():\n",
    "        info = ' '.join(data[data['QQ'] == qq]['Info'])\n",
    "        str = '%s,%s\\n' % (qq, info)\n",
    "        f_output.write(str)\n",
    "    f_output.close()\n",
    "\n",
    "\n",
    "def export_perplexity1(corpus_tfidf, dictionary, corpus):\n",
    "    lp1 = []\n",
    "    lp2 = []\n",
    "    topic_nums = np.arange(2, 51)\n",
    "    for t in topic_nums:\n",
    "        model = models.LdaModel(corpus_tfidf, num_topics=t, id2word=dictionary,\n",
    "                                alpha=0.001, eta=0.02, minimum_probability=0,\n",
    "                                update_every=1, chunksize=1000, passes=20)\n",
    "        lp = model.log_perplexity(corpus)\n",
    "        print 't = ', t,\n",
    "        print 'lda.log_perplexity(corpus) = ', lp,\n",
    "        lp1.append(lp)\n",
    "\n",
    "        lp = model.log_perplexity(corpus_tfidf)\n",
    "        print '\\t lda.log_perplexity(corpus_tfidf) = ', lp\n",
    "        lp2.append(lp)\n",
    "    print lp1\n",
    "    print lp2\n",
    "    column_names = 'Topic', 'Perplexity_Corpus', 'Perplexity_TFIDF'\n",
    "    perplexity_topic = pd.DataFrame(data=zip(topic_nums, lp1, lp2), columns=column_names)\n",
    "    perplexity_topic.to_csv('perplexity.csv', header=True, index=False)\n",
    "\n",
    "\n",
    "def export_perplexity2(corpus_tfidf, dictionary, corpus):\n",
    "    lp1 = []\n",
    "    lp2 = []\n",
    "    t = 20\n",
    "    passes = np.arange(1, 20)\n",
    "    for p in passes:\n",
    "        model = models.LdaModel(corpus_tfidf, num_topics=t, id2word=dictionary,\n",
    "                                alpha=0.001, eta=0.02, minimum_probability=0,\n",
    "                                update_every=1, chunksize=100, passes=p)\n",
    "        lp = model.log_perplexity(corpus)\n",
    "        print 't = ', t,\n",
    "        print 'lda.log_perplexity(corpus) = ', lp,\n",
    "        lp1.append(lp)\n",
    "\n",
    "        lp = model.log_perplexity(corpus_tfidf)\n",
    "        print '\\t lda.log_perplexity(corpus_tfidf) = ', lp\n",
    "        lp2.append(lp)\n",
    "    print lp1\n",
    "    print lp2\n",
    "    column_names = 'Passes', 'Perplexity_Corpus', 'Perplexity_TFIDF'\n",
    "    perplexity_topic = pd.DataFrame(data=zip(passes, lp1, lp2), columns=column_names)\n",
    "    perplexity_topic.to_csv('perplexity2.csv', header=True, index=False)\n",
    "\n",
    "\n",
    "def lda(export_perplexity=False):\n",
    "    np.set_printoptions(linewidth=300)\n",
    "    data = pd.read_csv('QQ_chat_result.csv', header=0)\n",
    "    texts = []\n",
    "    for info in data['Info']:\n",
    "        texts.append(info.decode('utf-8').split(' '))\n",
    "    M = len(texts)\n",
    "    print '文档数目：%d个' % M\n",
    "    # pprint(texts)\n",
    "\n",
    "    print '正在建立词典 --'\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    V = len(dictionary)\n",
    "    print '正在计算文本向量 --'\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    print '正在计算文档TF-IDF --'\n",
    "    t_start = time.time()\n",
    "    corpus_tfidf = models.TfidfModel(corpus)[corpus]\n",
    "    print '建立文档TF-IDF完成，用时%.3f秒' % (time.time() - t_start)\n",
    "    print 'LDA模型拟合推断 --'\n",
    "    num_topics = 20\n",
    "    t_start = time.time()\n",
    "    lda = models.LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary,\n",
    "                          alpha=0.001, eta=0.02, minimum_probability=0,\n",
    "                          update_every=1, chunksize=1000, passes=20)\n",
    "    print u'LDA模型完成，训练时间为\\t%.3f秒' % (time.time() - t_start)\n",
    "    if export_perplexity:\n",
    "        export_perplexity1(corpus_tfidf, dictionary, corpus)\n",
    "        # export_perplexity2(corpus_tfidf, dictionary, corpus)\n",
    "    # # 所有文档的主题\n",
    "    # doc_topic = [a for a in lda[corpus_tfidf]]\n",
    "    # print 'Document-Topic:\\n'\n",
    "    # pprint(doc_topic)\n",
    "\n",
    "    num_show_term = 7  # 每个主题显示几个词\n",
    "    print u'每个主题的词分布：'\n",
    "    for topic_id in range(num_topics):\n",
    "        print u'主题#%d：\\t' % topic_id,\n",
    "        term_distribute_all = lda.get_topic_terms(topicid=topic_id)\n",
    "        term_distribute = term_distribute_all[:num_show_term]\n",
    "        term_distribute = np.array(term_distribute)\n",
    "        term_id = term_distribute[:, 0].astype(np.int)\n",
    "        for t in term_id:\n",
    "            print dictionary.id2token[t],\n",
    "        print u'\\n概率：\\t', term_distribute[:, 1]\n",
    "\n",
    "    # 随机打印某10个文档的主题\n",
    "    np.set_printoptions(linewidth=200, suppress=True)\n",
    "    num_show_topic = 10  # 每个文档显示前几个主题\n",
    "    print u'10个用户的主题分布：'\n",
    "    doc_topics = lda.get_document_topics(corpus_tfidf)  # 所有文档的主题分布\n",
    "    idx = np.arange(M)\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:10]\n",
    "    for i in idx:\n",
    "        topic = np.array(doc_topics[i])\n",
    "        topic_distribute = np.array(topic[:, 1])\n",
    "        # print topic_distribute\n",
    "        topic_idx = topic_distribute.argsort()[:-num_show_topic - 1:-1]\n",
    "        print (u'第%d个用户的前%d个主题：' % (i, num_show_topic)), topic_idx\n",
    "        print topic_distribute[topic_idx]\n",
    "    # 显示着10个文档的主题\n",
    "    # mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "    # mpl.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(12, 9), facecolor='w')\n",
    "    for i, k in enumerate(idx):\n",
    "        ax = plt.subplot(5, 2, i + 1)\n",
    "        topic = np.array(doc_topics[i])\n",
    "        topic_distribute = np.array(topic[:, 1])\n",
    "        ax.stem(topic_distribute, linefmt='g-', markerfmt='ro')\n",
    "        ax.set_xlim(-1, num_topics + 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_ylabel(u\"概率\")\n",
    "        ax.set_title(u\"用户 {}\".format(k))\n",
    "        ax.grid(b=True)\n",
    "    plt.xlabel(u\"主题\", fontsize=14)\n",
    "    plt.suptitle(u'用户的主题分布', fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "    # 计算各个主题的强度\n",
    "    print u'\\n各个主题的强度:\\n'\n",
    "    topic_all = np.zeros(num_topics)\n",
    "    doc_topics = lda.get_document_topics(corpus_tfidf)  # 所有文档的主题分布\n",
    "    for i in np.arange(M):  # 遍历所有文档\n",
    "        topic = np.array(doc_topics[i])\n",
    "        topic_distribute = np.array(topic[:, 1])\n",
    "        topic_all += topic_distribute\n",
    "    topic_all /= M  # 平均\n",
    "    idx = topic_all.argsort()\n",
    "    topic_sort = topic_all[idx]\n",
    "    print topic_sort\n",
    "    plt.figure(facecolor='w')\n",
    "    plt.stem(topic_sort, linefmt='g-', markerfmt='ro')\n",
    "    plt.xticks(np.arange(idx.size), idx)\n",
    "    plt.xlabel(u\"主题\", fontsize=14)\n",
    "    plt.ylabel(u\"主题出现概率\", fontsize=14)\n",
    "    plt.title(u'主题强度', fontsize=18)\n",
    "    plt.grid(b=True, axis='both')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_perplexity():\n",
    "    data = pd.read_csv('Perplexity2.csv', header=0)\n",
    "    print data\n",
    "    columns = list(data.columns)\n",
    "    mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "    mpl.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(facecolor='w')\n",
    "    plt.plot(data[columns[0]], data[columns[1]], 'ro-', lw=2, ms=6, label='Log Perplexity(Corpus)')\n",
    "    # plt.plot(data[columns[0]], data[columns[2]], 'go--', lw=2, ms=6, label='Log Perplexity(TFIDF)')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.xlabel(columns[0], fontsize=16)\n",
    "    plt.ylabel(columns[1], fontsize=16)\n",
    "    plt.title(u'Perplexity', fontsize=18)\n",
    "    plt.grid(b=True, ls=':')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularize_data\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: u'\\u673a\\u5668\\u5b66\\u4e60\\u5347\\u7ea7\\u7248IV.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ff32e5976127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'regularize_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mregularize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'机器学习升级版IV.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'segment'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5b8e0d035151>\u001b[0m in \u001b[0;36mregularize_data\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mqq_pattern1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([1-9]\\d{4,15})'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# QQ号最小是10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mqq_pattern2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)*)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mf_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'QQ_chat.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mf_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'QQ,Time,Info\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: u'\\u673a\\u5668\\u5b66\\u4e60\\u5347\\u7ea7\\u7248IV.txt'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print 'regularize_data'\n",
    "    regularize_data(u'机器学习升级版IV.txt')\n",
    "    print 'segment'\n",
    "    segment()\n",
    "    print 'combine'\n",
    "    combine()\n",
    "    print 'lda'\n",
    "    lda(export_perplexity=True)\n",
    "    # show_perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
